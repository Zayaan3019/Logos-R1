# Reason-1: System 2 Reasoning Model

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.1+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Reason-1** is a state-of-the-art System 2 reasoning model that learns to "think before answering" using **Group Relative Policy Optimization (GRPO)**, a novel reinforcement learning algorithm. The model generates explicit reasoning chains in `<think>...</think>` tags and is trained to maximize mathematical problem-solving accuracy on GSM8K.

## ğŸš€ Key Features

- **GRPO Algorithm**: Efficient RL training without a value function (critic-free PPO variant)
- **Process Reward Model (PRM)**: Deterministic math verification with symbolic equivalence checking
- **LoRA/QLoRA**: Parameter-efficient fine-tuning for memory-constrained environments
- **StreamingLLM**: KV cache management for 4k+ token reasoning chains
- **Best-of-N Inference**: Rejection sampling with reward-based selection
- **Self-Correction**: Model learns to backtrack and verify its reasoning

## ğŸ“Š Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Reason-1 Architecture                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Input: "Solve: John has 5 apples..."                       â”‚
â”‚     â†“                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Policy Ï€Î¸ (Llama-3-8B + LoRA)     â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚     â†“                                                        â”‚
â”‚  <think>                                                     â”‚
â”‚    Step 1: John starts with 5 apples                        â”‚
â”‚    Step 2: He buys 3 more, so 5 + 3 = 8                     â”‚
â”‚    Step 3: Final count is 8 apples                          â”‚
â”‚  </think>                                                    â”‚
â”‚  The answer is 8                                            â”‚
â”‚     â†“                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Reward Function (Math Verifier)    â”‚                   â”‚
â”‚  â”‚   â€¢ Format: +0.1 (has <think> tags)  â”‚                   â”‚
â”‚  â”‚   â€¢ Answer: +1.0 (correct)           â”‚                   â”‚
â”‚  â”‚   â€¢ Quality: -0.0 (no repetition)    â”‚                   â”‚
â”‚  â”‚   â†’ Total Reward: 1.1                â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚     â†“                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚       GRPO Trainer                   â”‚                   â”‚
â”‚  â”‚   1. Sample G=4 outputs per prompt   â”‚                   â”‚
â”‚  â”‚   2. Compute group-relative advantagesâ”‚                   â”‚
â”‚  â”‚   3. Update policy with PPO-clip     â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.9+
- CUDA 11.8+ (for GPU training)
- 24GB+ GPU VRAM (for Llama-3-8B with LoRA; use QLoRA for less)

### Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/Reason-1.git
cd Reason-1

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# (Optional) Install Flash Attention for faster training
pip install flash-attn --no-build-isolation

# (Optional) Install vLLM for high-throughput inference
pip install vllm
```

## ğŸ“š Project Structure

```
Reason-1/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py              # Hyperparameter configurations
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ loader.py          # GSM8K dataset loader
â”‚   â”‚   â””â”€â”€ tokenizer.py       # Custom tokenizer with <think> tags
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ policy.py          # Actor model with LoRA
â”‚   â”‚   â””â”€â”€ reward.py          # Math verifier (THE JUDGE)
â”‚   â”œâ”€â”€ rl/
â”‚   â”‚   â”œâ”€â”€ grpo_trainer.py    # GRPO algorithm implementation
â”‚   â”‚   â””â”€â”€ buffer.py          # Experience replay buffer
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”œâ”€â”€ search.py          # Best-of-N sampling
â”‚   â”‚   â””â”€â”€ kv_cache.py        # StreamingLLM cache manager
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logging.py         # WandB integration
â”‚       â””â”€â”€ math_utils.py      # Answer extraction utilities
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_sft.py           # Stage 1: Supervised Fine-Tuning
â”‚   â””â”€â”€ train_rl.py            # Stage 2: GRPO RL Training
â”œâ”€â”€ tests/                     # Unit tests
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ“ Training Pipeline

### Stage 1: Supervised Fine-Tuning (SFT)

First, warm-start the model with reasoning traces:

```bash
python scripts/train_sft.py \
  --model_name meta-llama/Llama-3-8B \
  --epochs 3 \
  --batch_size 4 \
  --learning_rate 2e-5 \
  --output_dir ./outputs/sft \
  --use_amp
```

**What this does:**
- Fine-tunes the base model on GSM8K with `<think>` formatting
- Teaches the model to generate step-by-step reasoning
- Saves checkpoints to `./outputs/sft/`

### Stage 2: GRPO Reinforcement Learning

Train with RL to maximize reward:

```bash
python scripts/train_rl.py \
  --sft_model ./outputs/sft/best_model \
  --epochs 5 \
  --batch_size 4 \
  --group_size 4 \
  --learning_rate 1e-5 \
  --kl_coef 0.05 \
  --updates_per_batch 4 \
  --output_dir ./outputs/rl
```

**What this does:**
- Loads the SFT model as initialization
- Samples G=4 outputs per prompt
- Computes group-relative advantages (no critic needed!)
- Updates policy to favor high-reward reasoning chains
- Monitors KL divergence to prevent collapse

## ğŸ“Š Monitoring Training

Training metrics are automatically logged to [Weights & Biases](https://wandb.ai):

- **Reward Curves**: Mean/max/min rewards over time
- **KL Divergence**: Policy drift from reference
- **Loss Values**: Policy loss and total loss
- **Sample Outputs**: Generated reasoning traces

View your dashboard at: `https://wandb.ai/<your-entity>/reason-1`

## ğŸ”¬ GRPO Algorithm Details

### The Core Innovation

GRPO eliminates the need for a value function by using **group-relative advantages**:

```python
# Traditional PPO
Advantage = Q(s, a) - V(s)  # Requires value function V

# GRPO (Our Approach)
For each prompt q, sample G outputs: {o_1, ..., o_G}
Compute rewards: r_1, ..., r_G
Advantage_i = (r_i - mean(r)) / (std(r) + Îµ)  # No V needed!
```

### Algorithm Pseudocode

```python
for epoch in range(num_epochs):
    for prompt_batch in dataloader:
        # 1. Sample Group
        outputs = [policy.generate(prompt) for _ in range(G)]
        
        # 2. Compute Rewards
        rewards = [reward_fn(output, ground_truth) for output in outputs]
        
        # 3. Compute Advantages
        advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-8)
        
        # 4. PPO Update
        ratio = policy.prob(output) / old_policy.prob(output)
        clipped_ratio = clip(ratio, 1-Îµ, 1+Îµ)
        loss = -min(ratio * advantage, clipped_ratio * advantage)
        
        # 5. Add KL Penalty
        kl = KL(policy || reference_policy)
        total_loss = loss + Î² * kl
        
        # 6. Backward Pass
        total_loss.backward()
        optimizer.step()
```

## ğŸ§ª Inference

### Best-of-N Sampling

```python
from src.inference.search import create_sampler
from src.models.reward import create_reward_function

# Create sampler
sampler = create_sampler(
    model=policy,
    tokenizer=tokenizer,
    reward_function=reward_fn,
    strategy="best_of_n",
    num_samples=8,
)

# Generate with search
result = sampler.search(
    prompt="Solve: John has 5 apples and buys 3 more. How many does he have?",
    ground_truth="8",
)

print(f"Best Answer: {result.answer}")
print(f"Reasoning:\n{result.reasoning}")
print(f"Score: {result.best_score}")
```

### Beam Search (Alternative)

```python
sampler = create_sampler(
    model=policy,
    tokenizer=tokenizer,
    reward_function=reward_fn,
    strategy="beam_search",
    num_samples=4,
)
```

## ğŸ¯ Reward Function Design

The reward function is **CRITICAL** - if it's buggy, RL will fail.

### Components

1. **Format Reward** (+0.1): Uses `<think>` tags correctly
2. **Answer Reward** (+1.0): Final answer matches ground truth
3. **Length Penalty** (-0.001/token): Prevents verbosity
4. **Repetition Penalty** (-0.1): Penalizes repetitive reasoning

### Verification Strategy

```python
# 1. Symbolic Equivalence (preferred)
are_equivalent("5/10", "1/2")  # True (using sympy)

# 2. Numerical Tolerance
abs(float(pred) - float(gt)) < 1e-6

# 3. String Matching (fallback)
normalize("1,234.00") == normalize("1234")  # True
```

## ğŸ“ˆ Expected Results

After full training (SFT + GRPO), expect:

| Metric | Value |
|--------|-------|
| GSM8K Accuracy | 70-80% |
| Average Reward | 0.9+ |
| Reasoning Length | 150-300 tokens |
| Self-Correction Rate | 15-20% |

## ğŸ› Troubleshooting

### Out of Memory (OOM)

```bash
# Option 1: Smaller batch size
--batch_size 2 --group_size 2

# Option 2: Gradient accumulation
--gradient_accumulation_steps 4

# Option 3: Use smaller model
--model_name Qwen/Qwen2.5-3B-Instruct
```

### KL Divergence Exploding

```bash
# Increase KL coefficient
--kl_coef 0.1  # (default: 0.05)
```

### Reward Not Improving

1. Check reward function with unit tests
2. Verify SFT model quality
3. Reduce learning rate
4. Ensure dataset is correct

## ğŸ§ª Testing

Run unit tests for the reward function:

```bash
pytest tests/ -v
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **GRPO Algorithm**: Inspired by Group Relative Policy Optimization techniques
- **GSM8K Dataset**: [OpenAI GSM8K](https://github.com/openai/grade-school-math)
- **Base Models**: Meta LLaMA-3, Qwen-2.5
- **Libraries**: HuggingFace Transformers, TRL, PEFT

## ğŸ“š Citation

If you use this code in your research, please cite:

```bibtex
@software{reason1_2026,
  title={Reason-1: A System 2 Reasoning Model with GRPO},
  author={Your Name},
  year={2026},
  url={https://github.com/yourusername/Reason-1}
}
```

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“§ Contact

For questions or collaboration:
- Email: your.email@example.com
- Twitter: [@yourusername](https://twitter.com/yourusername)
- GitHub Issues: [Create an issue](https://github.com/yourusername/Reason-1/issues)

---

**Built with â¤ï¸ for advancing AI reasoning capabilities**
